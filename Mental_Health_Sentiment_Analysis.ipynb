{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mental Health Sentiment Analysis\n",
    "\n",
    "This notebook trains a sentiment analysis model for mental health text using DistilBERT.\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets scikit-learn pandas torch streamlit gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download dataset from Google Drive\n",
    "# Dataset ID: 1B5QpclAWO_x78sYTx63Q05yv1kwc_hno\n",
    "dataset_file = \"rmhd_labeled_merged.csv\"\n",
    "\n",
    "# Check if running in Colab\n",
    "is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "    !pip install -q gdown\n",
    "    !gdown 1B5QpclAWO_x78sYTx63Q05yv1kwc_hno -O {dataset_file}\n",
    "    file_path = f\"/content/{dataset_file}\"\n",
    "else:\n",
    "    file_path = dataset_file\n",
    "\n",
    "# Load and prepare the dataset\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"Dataset file not found at {file_path}. Please download it first.\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle different possible column names\n",
    "if 'selftext' in df.columns:\n",
    "    df = df.rename(columns={\"selftext\": \"text\"})\n",
    "elif 'Text' in df.columns:\n",
    "    df = df.rename(columns={\"Text\": \"text\"})\n",
    "\n",
    "if 'Label' in df.columns:\n",
    "    df = df.rename(columns={\"Label\": \"label\"})\n",
    "\n",
    "# Clean the dataset\n",
    "df = df[[\"text\", \"label\"]].dropna()\n",
    "df = df[df['text'].str.strip() != '']  # Remove blank rows\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df[\"labels\"] = le.fit_transform(df[\"label\"])\n",
    "label_names = list(le.classes_)\n",
    "num_labels = len(label_names)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"Label names: {label_names}\")\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert to Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"labels\"]])\n",
    "\n",
    "# Train-test split\n",
    "tokenized_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Train samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Test samples: {len(tokenized_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Use DistilBERT for faster training and inference\n",
    "model_id = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "print(f\"Model and tokenizer loaded: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove text column and set format\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"Dataset tokenized successfully!\")\n",
    "print(f\"Sample tokenized data: {tokenized_dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./sentiment_model\"\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save label mapping for later use\n",
    "import json\n",
    "label_map = {i: label for i, label in enumerate(label_names)}\n",
    "with open(f\"{output_dir}/label_map.json\", \"w\") as f:\n",
    "    json.dump(label_map, f, indent=2)\n",
    "\n",
    "print(f\"Model, tokenizer, and label map saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "output_dir = \"./sentiment_model\"\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Load label map\n",
    "with open(f\"{output_dir}/label_map.json\", \"r\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a given text using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text for sentiment analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: The predicted sentiment label.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = loaded_tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "    \n",
    "    # Get the predicted label (index with the highest score)\n",
    "    predicted_label_id = torch.argmax(outputs.logits).item()\n",
    "    \n",
    "    # Map the label ID back to the sentiment label string\n",
    "    predicted_sentiment = label_map.get(str(predicted_label_id), \"Unknown Label\")\n",
    "    \n",
    "    return predicted_sentiment\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"I am feeling very anxious and stressed about my exams.\",\n",
    "    \"This is the best day of my life!\",\n",
    "    \"I've been struggling with depression lately.\"\n",
    "]\n",
    "\n",
    "print(\"Testing the model:\")\n",
    "print(\"=\" * 50)\n",
    "for text in test_texts:\n",
    "    predicted_sentiment = predict_sentiment(text)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Predicted Sentiment: {predicted_sentiment}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}