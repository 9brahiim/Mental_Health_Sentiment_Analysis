{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mental Health Sentiment Analysis\n",
    "\n",
    "This notebook trains a sentiment analysis model for mental health text using LLaMA 2.\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "**Important:** LLaMA 2 requires a Hugging Face account and access token. \n",
    "1. Create an account at https://huggingface.co\n",
    "2. Request access to LLaMA 2 at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "3. Create a token at https://huggingface.co/settings/tokens\n",
    "4. In Google Colab: Add the token as a secret named `HF_TOKEN`\n",
    "5. Locally: Set it as environment variable or use `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets scikit-learn pandas torch streamlit gdown accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download dataset from Google Drive\n",
    "# Dataset ID: 1B5QpclAWO_x78sYTx63Q05yv1kwc_hno\n",
    "dataset_file = \"rmhd_labeled_merged.csv\"\n",
    "\n",
    "# Check if running in Colab\n",
    "is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "    !pip install -q gdown\n",
    "    !gdown 1B5QpclAWO_x78sYTx63Q05yv1kwc_hno -O {dataset_file}\n",
    "    file_path = f\"/content/{dataset_file}\"\n",
    "else:\n",
    "    file_path = dataset_file\n",
    "\n",
    "# Load and prepare the dataset\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"Dataset file not found at {file_path}. Please download it first.\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle different possible column names\n",
    "if 'selftext' in df.columns:\n",
    "    df = df.rename(columns={\"selftext\": \"text\"})\n",
    "elif 'Text' in df.columns:\n",
    "    df = df.rename(columns={\"Text\": \"text\"})\n",
    "\n",
    "if 'Label' in df.columns:\n",
    "    df = df.rename(columns={\"Label\": \"label\"})\n",
    "\n",
    "# Clean the dataset\n",
    "df = df[[\"text\", \"label\"]].dropna()\n",
    "df = df[df['text'].str.strip() != '']  # Remove blank rows\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df[\"labels\"] = le.fit_transform(df[\"label\"])\n",
    "label_names = list(le.classes_)\n",
    "num_labels = len(label_names)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"Label names: {label_names}\")\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert to Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"labels\"]])\n",
    "\n",
    "# Train-test split\n",
    "tokenized_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Train samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Test samples: {len(tokenized_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Hugging Face Authentication\n",
    "\n",
    "**Note:** LLaMA 2 requires authentication. Make sure you have:\n",
    "- Accepted the model license on Hugging Face\n",
    "- Created a token at https://huggingface.co/settings/tokens\n",
    "- Added it to Colab Secrets (name: `HF_TOKEN`) or set as environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "# LLaMA 2 model ID\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Setup Hugging Face authentication\n",
    "is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "    # Try to get token from Colab secrets\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        hf_token = userdata.get('HF_TOKEN')\n",
    "        if hf_token:\n",
    "            login(token=hf_token)\n",
    "            print(\"✓ Authenticated with Hugging Face using Colab secret\")\n",
    "        else:\n",
    "            print(\"⚠️ HF_TOKEN not found in Colab secrets. Please add it in Colab Secrets.\")\n",
    "            print(\"You can also manually login using: login(token='your_token_here')\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Not in Colab environment. Please set HF_TOKEN environment variable or use login()\")\n",
    "else:\n",
    "    # For local environment, try to get from environment variable\n",
    "    hf_token = os.environ.get('HF_TOKEN')\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"✓ Authenticated with Hugging Face using environment variable\")\n",
    "    else:\n",
    "        print(\"⚠️ HF_TOKEN not found. Please set it as environment variable or use login()\")\n",
    "        print(\"You can run: huggingface-cli login\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer for {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# Set pad token if it doesn't exist (LLaMA doesn't have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load model with quantization for memory efficiency (optional but recommended)\n",
    "print(f\"Loading model {model_id}...\")\n",
    "print(\"Note: This may take a few minutes and requires significant GPU memory.\")\n",
    "\n",
    "try:\n",
    "    # Configure 4-bit quantization to reduce memory usage\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=num_labels,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"✓ Model loaded with 4-bit quantization\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not load with quantization: {e}\")\n",
    "    print(\"Loading without quantization (requires more memory)...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=num_labels,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "print(f\"✓ Model and tokenizer loaded: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    # LLaMA 2 works better with longer sequences, but we'll keep 256 for consistency\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove text column and set format\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"Dataset tokenized successfully!\")\n",
    "print(f\"Sample tokenized data: {tokenized_dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# LLaMA 2 requires smaller batch sizes due to memory constraints\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,  # Reduced for LLaMA 2 (was 8)\n",
    "    per_device_eval_batch_size=2,   # Reduced for LLaMA 2 (was 8)\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "    fp16=True,  # Use mixed precision for LLaMA 2\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=100\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured for LLaMA 2!\")\n",
    "print(\"Note: Training LLaMA 2 requires significant GPU memory and time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./sentiment_model\"\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save label mapping for later use\n",
    "import json\n",
    "label_map = {i: label for i, label in enumerate(label_names)}\n",
    "with open(f\"{output_dir}/label_map.json\", \"w\") as f:\n",
    "    json.dump(label_map, f, indent=2)\n",
    "\n",
    "print(f\"Model, tokenizer, and label map saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "output_dir = \"./sentiment_model\"\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Load label map\n",
    "with open(f\"{output_dir}/label_map.json\", \"r\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a given text using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text for sentiment analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: The predicted sentiment label.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = loaded_tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "    \n",
    "    # Get the predicted label (index with the highest score)\n",
    "    predicted_label_id = torch.argmax(outputs.logits).item()\n",
    "    \n",
    "    # Map the label ID back to the sentiment label string\n",
    "    predicted_sentiment = label_map.get(str(predicted_label_id), \"Unknown Label\")\n",
    "    \n",
    "    return predicted_sentiment\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"I am feeling very anxious and stressed about my exams.\",\n",
    "    \"This is the best day of my life!\",\n",
    "    \"I've been struggling with depression lately.\"\n",
    "]\n",
    "\n",
    "print(\"Testing the model:\")\n",
    "print(\"=\" * 50)\n",
    "for text in test_texts:\n",
    "    predicted_sentiment = predict_sentiment(text)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Predicted Sentiment: {predicted_sentiment}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
